{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CORD-19 Dataset Analysis\n",
    "# Analysis of Bias in COVID-19 Research Publications\n",
    "\n",
    "# %% [markdown]\n",
    "# # 1. Setup and Data Loading\n",
    "# First, we'll import necessary libraries and load our dataset. We'll also perform initial data cleaning and preparation.\n",
    "\n",
    "# %%\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.1 Load and Examine Data\n",
    "# We'll load the CORD-19 dataset and examine its basic properties.\n",
    "\n",
    "# %%\n",
    "# Load the dataset with specified data types\n",
    "dtype_dict = {\n",
    "    'sha': str,\n",
    "    'doi': str,\n",
    "    'pmcid': str,\n",
    "    'pubmed_id': str,\n",
    "    'who_covidence_id': str,\n",
    "    'arxiv_id': str,\n",
    "    'pdf_json_files': str,\n",
    "    'pmc_json_files': str\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('metadata.csv', dtype=dtype_dict)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# %% [markdown]\n",
    "# # 2. Journal Analysis\n",
    "# We'll analyze the distribution of publications across journals, including impact factors and accessibility.\n",
    "\n",
    "# %%\n",
    "# Analyze journal distribution\n",
    "journal_counts = df['journal'].value_counts()\n",
    "\n",
    "# Create a bar plot of top journals\n",
    "plt.figure(figsize=(15, 8))\n",
    "journal_counts.head(20).plot(kind='bar')\n",
    "plt.title('Top 20 Journals by Number of Publications')\n",
    "plt.xlabel('Journal')\n",
    "plt.ylabel('Number of Publications')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.1 Journal Impact Analysis\n",
    "# Let's analyze the relationship between journal impact factors and publication counts.\n",
    "# Note: You'll need to add a dictionary of journal impact factors\n",
    "\n",
    "# %%\n",
    "# Create a dictionary of example impact factors (you should replace with actual data)\n",
    "impact_factors = {\n",
    "    'Nature': 49.962,\n",
    "    'Science': 41.845,\n",
    "    'The Lancet': 79.321,\n",
    "    'PLOS ONE': 3.240,\n",
    "    'bioRxiv': 'Preprint',\n",
    "    'medRxiv': 'Preprint'\n",
    "}\n",
    "\n",
    "# %% [markdown]\n",
    "# # 3. Temporal Analysis\n",
    "# Analyze how publication patterns changed over time\n",
    "\n",
    "# %%\n",
    "# Convert publish_time to datetime\n",
    "df['publish_time'] = pd.to_datetime(df['publish_time'], errors='coerce')\n",
    "df['year'] = df['publish_time'].dt.year\n",
    "df['month'] = df['publish_time'].dt.month\n",
    "\n",
    "# Create monthly publication counts\n",
    "monthly_counts = df.groupby([df['publish_time'].dt.year, \n",
    "                           df['publish_time'].dt.month]).size()\n",
    "\n",
    "# Plot temporal distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "monthly_counts.plot(kind='line')\n",
    "plt.title('Number of Publications Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Publications')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # 4. Accessibility Analysis\n",
    "# Analyze the accessibility of research papers\n",
    "\n",
    "# %%\n",
    "# Calculate accessibility metrics\n",
    "accessibility_metrics = {\n",
    "    'Has URL': (~df['url'].isna()).mean() * 100,\n",
    "    'Has PDF': (~df['pdf_json_files'].isna()).mean() * 100,\n",
    "    'Has PMC': (~df['pmc_json_files'].isna()).mean() * 100,\n",
    "    'Has DOI': (~df['doi'].isna()).mean() * 100\n",
    "}\n",
    "\n",
    "# Create bar plot of accessibility metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accessibility_metrics.keys(), accessibility_metrics.values())\n",
    "plt.title('Research Accessibility Metrics')\n",
    "plt.ylabel('Percentage of Papers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # 5. Topic Analysis\n",
    "# Perform topic modeling to understand research focus areas\n",
    "\n",
    "# %%\n",
    "# Prepare text data for topic modeling\n",
    "titles = df['title'].dropna()\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(titles)\n",
    "\n",
    "# Perform LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display top words for each topic\n",
    "def print_topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print()\n",
    "\n",
    "print_topics(lda, vectorizer.get_feature_names_out(), 10)\n",
    "\n",
    "# %% [markdown]\n",
    "# # 6. Statistical Analysis and Validation\n",
    "# Perform statistical tests to validate our findings\n",
    "\n",
    "# %%\n",
    "# Example: Chi-square test for journal distribution\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create contingency table\n",
    "journal_type = pd.crosstab(df['journal'].notna(), df['pdf_json_files'].notna())\n",
    "chi2, p_value, dof, expected = chi2_contingency(journal_type)\n",
    "\n",
    "print(\"Chi-square test results:\")\n",
    "print(f\"Chi-square statistic: {chi2}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
