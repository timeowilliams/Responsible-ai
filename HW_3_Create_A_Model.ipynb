{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timeowilliams/Responsible-ai/blob/main/HW_3_Create_A_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbdP0r8thmx"
      },
      "source": [
        "# CORD-19 Predictive Model\n",
        "## Responsible AI Assignment: Assignment 3 - Model Creation\n",
        "\n",
        "This notebook builds a predictive model to classify whether a CORD-19 paper has a PDF available (`pdf_json_files` not null). The dataset has biases (e.g., journal concentration, recent papers), which is okay per the assignment. We’ll fetch the data from Kaggle, install dependencies, clean it, train a model, evaluate it with detailed metrics, check for proxy features, and document everything for replication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XpbakEOthmz"
      },
      "source": [
        "## 1. Install Dependencies\n",
        "Install all required packages to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIQKtDX5thm0"
      },
      "outputs": [],
      "source": [
        "# Install dependencies from requirements.txt\n",
        "pip install pandas numpy matplotlib seaborn scikit-learn langdetect scipy statsmodels kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KizCZfakthm0"
      },
      "source": [
        "## 2. Setup and Data Loading\n",
        "Download the dataset from Kaggle and load it. Ensures data is clean and accurate.\n",
        "\n",
        "**Note**: You’ll need a Kaggle API key. See Section 8 for setup instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPr922Rxthm1"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from scipy.sparse import hstack\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Download dataset from Kaggle\n",
        "os.environ['KAGGLE_USERNAME'] = 'timeowilliams'\n",
        "os.environ['KAGGLE_KEY'] = 'a7f8c8f6ad2f54d8ce119b3d607e0833'\n",
        "!kaggle datasets download -d allen-institute-for-ai/CORD-19-research-challenge -f metadata.csv\n",
        "!unzip -o metadata.csv.zip\n",
        "\n",
        "# Load dataset\n",
        "dtype_dict = {\n",
        "    'sha': str,\n",
        "    'doi': str,\n",
        "    'pmcid': str,\n",
        "    'pubmed_id': str,\n",
        "    'who_covidence_id': str,\n",
        "    'arxiv_id': str,\n",
        "    'pdf_json_files': str,\n",
        "    'pmc_json_files': str\n",
        "}\n",
        "df = pd.read_csv('metadata.csv', dtype=dtype_dict)\n",
        "\n",
        "# Clean data\n",
        "df = df.drop_duplicates(subset='sha', keep='first')\n",
        "print(\"Dataset Shape after deduplication:\", df.shape)\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7nXeVPKthm1"
      },
      "source": [
        "## 3. Data Preprocessing\n",
        "Prepare features and target. Target: `has_pdf` (1 = PDF available, 0 = not). Features: journal, year, title text.\n",
        "\n",
        "**Note**: If `has_pdf` is imbalanced (e.g., mostly 1s), the model might favor the majority class. We’ll check this below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPMBvycSthm1"
      },
      "outputs": [],
      "source": [
        "# Define target\n",
        "df['has_pdf'] = (~df['pdf_json_files'].isna()).astype(int)\n",
        "print(\"\\nTarget Distribution (proportion of 0s and 1s):\")\n",
        "print(df['has_pdf'].value_counts(normalize=True))\n",
        "\n",
        "# Feature engineering\n",
        "df['publish_time'] = pd.to_datetime(df['publish_time'], errors='coerce')\n",
        "df['year'] = df['publish_time'].dt.year.fillna(-1).astype(int)\n",
        "df['journal'] = df['journal'].fillna('Unknown')\n",
        "df['title'] = df['title'].fillna('')\n",
        "\n",
        "# Select features and target\n",
        "X = df[['journal', 'year', 'title']]\n",
        "y = df['has_pdf']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"\\nTrain Shape:\", X_train.shape, \"Test Shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yolc5SWbthm1"
      },
      "source": [
        "## 4. Model Pipeline\n",
        "Build a pipeline to process features and train a Logistic Regression model.\n",
        "\n",
        "**Why Logistic Regression?** It’s simple, interpretable, and good for binary classification (0 or 1).\n",
        "\n",
        "**Note**: Encode `journal` on the full dataset to avoid unseen labels in test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72Goi5wRthm1"
      },
      "outputs": [],
      "source": [
        "# Encode journal on full dataset to handle all possible categories\n",
        "le = LabelEncoder()\n",
        "X['journal_encoded'] = le.fit_transform(X['journal'])  # Fit on full X before split\n",
        "X_train['journal_encoded'] = le.transform(X_train['journal'])  # Transform train\n",
        "X_test['journal_encoded'] = le.transform(X_test['journal'])    # Transform test\n",
        "\n",
        "# Vectorize title\n",
        "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_train_tfidf = tfidf.fit_transform(X_train['title'])\n",
        "X_test_tfidf = tfidf.transform(X_test['title'])\n",
        "\n",
        "# Combine features\n",
        "X_train_final = hstack([X_train_tfidf, X_train[['year', 'journal_encoded']].values])\n",
        "X_test_final = hstack([X_test_tfidf, X_test[['year', 'journal_encoded']].values])\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train_final, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test_final)\n",
        "print(\"\\nModel Training Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVLf3-xythm2"
      },
      "source": [
        "## 5. Model Evaluation\n",
        "Evaluate performance with metrics and visuals.\n",
        "\n",
        "**Quick Metrics Guide**:\n",
        "- **Confusion Matrix**: Counts predictions:\n",
        "  - True Positives (TP): Predicted 1, actual 1 (correctly predicted PDF)\n",
        "  - True Negatives (TN): Predicted 0, actual 0 (correctly predicted no PDF)\n",
        "  - False Positives (FP): Predicted 1, actual 0 (wrongly predicted PDF)\n",
        "  - False Negatives (FN): Predicted 0, actual 1 (missed a PDF)\n",
        "- **Precision**: TP / (TP + FP) - How often are positive predictions correct?\n",
        "- **Recall**: TP / (TP + FN) - How many actual positives did we catch?\n",
        "- **F1**: Balances precision and recall\n",
        "- **FPR**: FP / (FP + TN) - Rate of wrong positives\n",
        "- **FNR**: FN / (FN + TP) - Rate of missed positives\n",
        "- **TPR**: Same as recall\n",
        "- **TNR**: TN / (TN + FP) - Rate of correct negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX4c8hwEthm2"
      },
      "outputs": [],
      "source": [
        "# Basic metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Extended metrics (force 2x2 matrix with labels=[0, 1])\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()  # Now guaranteed to unpack 4 values\n",
        "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "print(f\"\\nExtended Metrics: FPR={fpr:.3f}, FNR={fnr:.3f}, TPR={tpr:.3f}, TNR={tnr:.3f}, PPV={ppv:.3f}\")\n",
        "\n",
        "# Group analysis by year\n",
        "df_test = X_test.copy()\n",
        "df_test['y_true'] = y_test\n",
        "df_test['y_pred'] = y_pred\n",
        "print(\"\\nFPR and TPR by Year (checking equal opportunity):\")\n",
        "for year in df_test['year'].unique():\n",
        "    subset = df_test[df_test['year'] == year]\n",
        "    tn, fp, fn, tp = confusion_matrix(subset['y_true'], subset['y_pred'], labels=[0, 1]).ravel()\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    print(f\"Year {year}: FPR={fpr:.3f}, TPR={tpr:.3f}\")\n",
        "\n",
        "# Confusion matrix plot\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoUmsy1Zthm2"
      },
      "source": [
        "## 6. Proxy Features Detection\n",
        "Check if features like `journal` or `year` act as proxies for sensitive attributes (e.g., prestige, region).\n",
        "\n",
        "**What’s a Proxy?** A feature that indirectly hints at something sensitive (e.g., `journal` might reflect funding levels). We’ll use correlation and VIF (Variance Inflation Factor) to spot overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI4HEFcPthm2"
      },
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "feature_names = tfidf.get_feature_names_out().tolist() + ['year', 'journal_encoded']\n",
        "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': model.coef_[0]})\n",
        "print(\"\\nTop 10 Positive Coefficients (Increase PDF Likelihood):\")\n",
        "print(coef_df.sort_values('Coefficient', ascending=False).head(10))\n",
        "print(\"\\nTop 10 Negative Coefficients (Decrease PDF Likelihood):\")\n",
        "print(coef_df.sort_values('Coefficient').head(10))\n",
        "\n",
        "# Proxy check: Correlation\n",
        "print(\"\\nCorrelation Matrix (year vs. journal_encoded):\")\n",
        "print(X_train[['year', 'journal_encoded']].corr())\n",
        "\n",
        "# Proxy check: VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = ['year', 'journal_encoded']\n",
        "vif_data['VIF'] = [variance_inflation_factor(X_train[['year', 'journal_encoded']].values, i)\n",
        "                   for i in range(2)]\n",
        "print(\"\\nVariance Inflation Factor (VIF > 5 suggests overlap):\")\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoUGEecnthm2"
      },
      "source": [
        "## 7. Analysis and Discussion\n",
        "### Model Performance\n",
        "- **Accuracy**: 1.000 (100%) reflects that all 74,744 test instances have PDFs, and the model predicts “1” for all, perfectly matching the test set. This stems from the dataset’s extreme imbalance (99.9997% PDFs, 0.0003% no PDFs), with the single “no PDF” case in the training set.\n",
        "- **Metrics**: TPR = 1.000 (caught all PDFs), FPR = 0.000, FNR = 0.000, TNR = 0.000, PPV = 1.000. Metrics for class 0 are undefined in practice (no `0`s in test), defaulting to 0 due to absence of TN, FP, or FN. The confusion matrix is [[0, 0], [0, 74744]], showing only TPs.\n",
        "- **Group Fairness**: FPR and TPR by year are uniformly 0.000 and 1.000, respectively, as all test data is class 1. No meaningful fairness analysis is possible without class 0 representation.\n",
        "\n",
        "### Proxy Insights\n",
        "- **Coefficients**: Positive coefficients (e.g., `year`=0.003899, `journal_encoded`=0.003849, `virus`=2.052750e-06) align with higher PDF availability, tied to recent years and COVID-related journals/topics. Negative terms (e.g., `scientific`=-1.377613e-05) slightly reduce likelihood, but all effects are muted with 99.9997% “1”s—the model defaults to “1” regardless.\n",
        "- **Correlation/VIF**: Correlation between `year` and `journal_encoded` is 0.006499 (negligible), and VIF values (4.041687) confirm no overlap, indicating no significant proxying between these features.\n",
        "\n",
        "### Bias Reflection\n",
        "The model is a trivial “always yes” predictor due to the dataset’s extreme skew (0.0003% no PDFs), reflecting CORD-19’s bias toward accessible, PDF-available research. With no “no PDF” cases in the test set, it achieves perfect accuracy but learns nothing beyond the majority class. This satisfies the assignment’s “bias is okay” rule, delivering predictable results, though it lacks practical value for distinguishing rare cases. Startups might prioritize such simplicity over nuance due to resource constraints.\n",
        "\n",
        "**Future Idea**: Proxy analysis could explore Ivy League funding or military lab proximity—fun projects needing balanced datasets!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W8G1S8Mthm2"
      },
      "source": [
        "## 8. Reproducibility Notes\n",
        "To replicate:\n",
        "1. Clone repo: `git clone https://github.com/timeowilliams/Responsible-ai`\n",
        "2. Run this notebook in Jupyter (installs dependencies and downloads `metadata.csv` automatically).\n",
        "3. Set up Kaggle API:\n",
        "   - Go to [Kaggle Account](https://www.kaggle.com/account), create API token (`kaggle.json`).\n",
        "   - In Section 2, `KAGGLE_USERNAME` and `KAGGLE_KEY` use `timeowilliams` and its key. Replace with your own if forking.\n",
        "   - Alternatively, run `kaggle config set -n username -v YOUR_USERNAME` and `kaggle config set -n key -v YOUR_KEY` in terminal.\n",
        "\n",
        "Random state is 42 for consistency. Dataset source: [Kaggle CORD-19](https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19-research-challenge?select=metadata.csv)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXvZHsGBthm2"
      },
      "source": [
        "## 9. Acknowledgments\n",
        "This notebook was developed with significant assistance from Grok, an AI developed by xAI. Grok provided guidance on data preprocessing, model development, error troubleshooting, and documentation to ensure reproducibility and alignment with Responsible AI principles for this assignment."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}